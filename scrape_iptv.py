import requests
import re
import os
from bs4 import BeautifulSoup
import time
import sys

def scrape_tonkiang():
    base_url = "https://tonkiang.us/iptvmulticast.php"
    
    # ä½¿ç”¨æ›´ç®€å•çš„headers
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    # åˆ›å»ºtonkiangæ–‡ä»¶å¤¹
    output_dir = "tonkiang"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    all_ips_by_province = {}
    total_ips = 0
    
    print("ğŸš€ å¼€å§‹æŠ“å–IPTV IPåœ°å€...")
    print(f"ç›®æ ‡URL: {base_url}")
    
    # æŠ“å–4é¡µæ•°æ®
    for page in range(1, 5):
        print(f"\nğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ...")
        
        url = f"https://tonkiang.us/iptvmulticast.php?page={page}&iphone16=&code="
        print(f"è¯·æ±‚URL: {url}")
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                # ä¿å­˜åŸå§‹HTMLç”¨äºè°ƒè¯•
                with open(f"debug_page_{page}.html", "w", encoding="utf-8") as f:
                    f.write(response.text)
                print(f"âœ… ç¬¬ {page} é¡µHTMLå·²ä¿å­˜åˆ° debug_page_{page}.html")
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è°ƒè¯•ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«IPçš„å¯èƒ½å…ƒç´ 
                print("ğŸ” æœç´¢åŒ…å«IPåœ°å€çš„å…ƒç´ ...")
                
                # æ–¹æ³•1ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«IPåœ°å€çš„æ–‡æœ¬
                ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
                all_text_ips = re.findall(ip_pattern, response.text)
                print(f"æ–‡æœ¬ä¸­æ‰¾åˆ°çš„IPæ•°é‡: {len(all_text_ips)}")
                if all_text_ips:
                    print(f"ç¤ºä¾‹IP: {all_text_ips[:5]}")
                
                # æ–¹æ³•2ï¼šæŸ¥æ‰¾ç‰¹å®šçš„HTMLç»“æ„
                result_divs = soup.find_all('div', class_='result')
                print(f"æ‰¾åˆ°çš„result divæ•°é‡: {len(result_divs)}")
                
                page_ip_count = 0
                
                for i, result in enumerate(result_divs):
                    print(f"  åˆ†æç¬¬ {i+1} ä¸ªresult div...")
                    
                    # æå–IPåœ°å€ - å¤šç§æ–¹æ³•å°è¯•
                    ip = None
                    province = "å…¶ä»–"
                    
                    # æ–¹æ³•1ï¼šä»channel divä¸­æå–
                    channel_div = result.find('div', class_='channel')
                    if channel_div:
                        channel_text = channel_div.get_text()
                        ip_matches = re.findall(ip_pattern, channel_text)
                        if ip_matches:
                            ip = ip_matches[0]
                            print(f"    ä»channelæ‰¾åˆ°IP: {ip}")
                    
                    # æ–¹æ³•2ï¼šä»æ‰€æœ‰é“¾æ¥ä¸­æå–
                    if not ip:
                        links = result.find_all('a')
                        for link in links:
                            href = link.get('href', '')
                            link_text = link.get_text()
                            # ä»hrefä¸­æå–IP
                            ip_matches = re.findall(ip_pattern, href)
                            if ip_matches:
                                ip = ip_matches[0]
                                print(f"    ä»é“¾æ¥hrefæ‰¾åˆ°IP: {ip}")
                                break
                            # ä»é“¾æ¥æ–‡æœ¬ä¸­æå–IP
                            ip_matches = re.findall(ip_pattern, link_text)
                            if ip_matches:
                                ip = ip_matches[0]
                                print(f"    ä»é“¾æ¥æ–‡æœ¬æ‰¾åˆ°IP: {ip}")
                                break
                    
                    # æ–¹æ³•3ï¼šä»æ•´ä¸ªresult divä¸­æå–
                    if not ip:
                        result_text = result.get_text()
                        ip_matches = re.findall(ip_pattern, result_text)
                        if ip_matches:
                            ip = ip_matches[0]
                            print(f"    ä»resultæ–‡æœ¬æ‰¾åˆ°IP: {ip}")
                    
                    if ip:
                        # æå–çœä»½ä¿¡æ¯
                        location_divs = result.find_all('div', style=re.compile(r'font-size: 11px; color: #aaa;'))
                        for location_div in location_divs:
                            location_text = location_div.get_text()
                            print(f"    ä½ç½®æ–‡æœ¬: {location_text}")
                            
                            # æå–çœä»½ä¿¡æ¯
                            province_matches = re.findall(r'([\u4e00-\u9fa5]{2,6}çœ|[\u4e00-\u9fa5]{2,4}å¸‚|[\u4e00-\u9fa5]{2,6}è‡ªæ²»åŒº)', location_text)
                            if province_matches:
                                province = province_matches[0]
                                print(f"    æ‰¾åˆ°çœä»½: {province}")
                                break
                        
                        # å°†IPæ·»åŠ åˆ°å¯¹åº”çœä»½çš„åˆ—è¡¨ä¸­
                        if province not in all_ips_by_province:
                            all_ips_by_province[province] = []
                        all_ips_by_province[province].append(ip)
                        page_ip_count += 1
                        total_ips += 1
                        print(f"    âœ… æˆåŠŸè®°å½•IP: {ip} - çœä»½: {province}")
                    else:
                        print(f"    âŒ ç¬¬ {i+1} ä¸ªresult divä¸­æœªæ‰¾åˆ°IP")
                
                print(f"  ğŸ“Š ç¬¬ {page} é¡µå…±æ‰¾åˆ° {page_ip_count} ä¸ªIP")
            else:
                print(f"  âŒ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                print(f"  å“åº”å†…å®¹å‰500å­—ç¬¦: {response.text[:500]}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
            
        except requests.RequestException as e:
            print(f"  âŒ ç¬¬ {page} é¡µè¯·æ±‚å¼‚å¸¸: {e}")
            continue
        except Exception as e:
            print(f"  âŒ ç¬¬ {page} é¡µå¤„ç†å¼‚å¸¸: {e}")
            continue
    
    print(f"\nğŸ“ˆ æŠ“å–ç»Ÿè®¡:")
    print(f"  æ€»IPæ•°é‡: {total_ips}")
    print(f"  çœä»½æ•°é‡: {len(all_ips_by_province)}")
    
    # å°†IPåœ°å€å†™å…¥å¯¹åº”çš„çœä»½æ–‡ä»¶
    files_created = 0
    for province, ips in all_ips_by_province.items():
        # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        safe_filename = re.sub(r'[<>:"/\\|?*]', '', province)
        file_path = os.path.join(output_dir, f"{safe_filename}.txt")
        
        # å»é‡å¹¶å†™å…¥æ–‡ä»¶
        unique_ips = list(set(ips))
        with open(file_path, 'w', encoding='utf-8') as f:
            for ip in unique_ips:
                f.write(ip + '\n')
        
        files_created += 1
        print(f"  ğŸ’¾ å·²ä¿å­˜ {len(unique_ips)} ä¸ªIPåˆ° {file_path}")
    
    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼")
    print(f"  åˆ›å»ºæ–‡ä»¶æ•°: {files_created}")
    print(f"  æ€»å”¯ä¸€IPæ•°: {total_ips}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•IPï¼Œä¿å­˜ä¸€ä¸ªæ ‡è®°æ–‡ä»¶
    if total_ips == 0:
        with open("no_ips_found.txt", "w") as f:
            f.write("æœ¬æ¬¡æŠ“å–æœªæ‰¾åˆ°ä»»ä½•IPåœ°å€\n")
        print("âŒ è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•IPåœ°å€")
    
    return total_ips, files_created

if __name__ == "__main__":
    try:
        total_ips, files_created = scrape_tonkiang()
        # é€€å‡ºç ç”¨äºGitHub Actionsåˆ¤æ–­
        if total_ips == 0:
            sys.exit(1)  # æ²¡æœ‰æ‰¾åˆ°IPï¼Œè¿”å›é”™è¯¯ç 
        else:
            sys.exit(0)  # æˆåŠŸæ‰¾åˆ°IP
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
