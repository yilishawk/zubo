import requests
import re
import os
import time
import sys
import random
from bs4 import BeautifulSoup

def scrape_tonkiang():
    """ä¸»æŠ“å–å‡½æ•°"""
    output_dir = "tonkiang"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    all_ips_by_province = {}
    total_ips = 0
    
    print("ğŸš€ å¼€å§‹æŠ“å–IPTV IPåœ°å€...")
    
    # å°è¯•å¤šç§æ–¹æ³•
    methods = [
        try_method_selenium_proxy,
        try_method_requests_advanced,
        try_method_simple_requests
    ]
    
    for method in methods:
        print(f"\nğŸ”„ å°è¯•æ–¹æ³•: {method.__name__}")
        result = method()
        if result and result[0] > 0:  # å¦‚æœæ‰¾åˆ°IP
            total_ips, files_created = result
            break
    else:
        # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
        print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
        total_ips, files_created = 0, 0
    
    return save_results(all_ips_by_province, total_ips)

def try_method_selenium_proxy():
    """æ–¹æ³•1: å°è¯•ä½¿ç”¨ä»£ç†å’Œé«˜çº§è¯·æ±‚å¤´"""
    print("ğŸ”§ ä½¿ç”¨é«˜çº§è¯·æ±‚å¤´å’Œæ–¹æ³•...")
    
    # è½®æ¢User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
    ]
    
    total_ips = 0
    
    for page in range(1, 5):
        print(f"  ğŸ“„ æŠ“å–ç¬¬ {page} é¡µ...")
        
        url = f"https://tonkiang.us/iptvmulticast.php?page={page}&iphone16=&code="
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-User': '?1',
            'Sec-Fetch-Dest': 'document',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
        }
        
        try:
            # ä½¿ç”¨ä¼šè¯ä¿æŒ
            session = requests.Session()
            session.headers.update(headers)
            
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(2, 5))
            
            response = session.get(url, timeout=20)
            
            if response.status_code == 200:
                # æ£€æŸ¥æ˜¯å¦è¢«é˜»æ­¢
                if any(blocked in response.text for blocked in ['Cloudflare', 'captcha', 'Checking your browser']):
                    print(f"    âŒ ç¬¬ {page} é¡µè¢«Cloudflareé˜»æ­¢")
                    continue
                
                # ä¿å­˜HTMLç”¨äºè°ƒè¯•
                with open(f"page_{page}_debug.html", "w", encoding="utf-8") as f:
                    f.write(response.text)
                
                soup = BeautifulSoup(response.text, 'html.parser')
                ips_found = parse_page(soup, {})
                total_ips += ips_found
                print(f"    âœ… ç¬¬ {page} é¡µæ‰¾åˆ° {ips_found} ä¸ªIP")
            else:
                print(f"    âŒ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
        except Exception as e:
            print(f"    âŒ ç¬¬ {page} é¡µå¼‚å¸¸: {e}")
            continue
    
    return total_ips

def try_method_requests_advanced():
    """æ–¹æ³•2: ä½¿ç”¨æ›´ç®€å•çš„è¯·æ±‚"""
    print("ğŸ”§ ä½¿ç”¨ç®€åŒ–è¯·æ±‚æ–¹æ³•...")
    
    total_ips = 0
    
    for page in range(1, 5):
        print(f"  ğŸ“„ æŠ“å–ç¬¬ {page} é¡µ...")
        
        url = f"https://tonkiang.us/iptvmulticast.php?page={page}&iphone16=&code="
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                ips_found = parse_page(soup, {})
                total_ips += ips_found
                print(f"    âœ… ç¬¬ {page} é¡µæ‰¾åˆ° {ips_found} ä¸ªIP")
            else:
                print(f"    âŒ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥")
                
            time.sleep(2)
            
        except Exception as e:
            print(f"    âŒ ç¬¬ {page} é¡µå¼‚å¸¸: {e}")
            continue
    
    return total_ips

def try_method_simple_requests():
    """æ–¹æ³•3: æœ€åŸºæœ¬çš„è¯·æ±‚"""
    print("ğŸ”§ ä½¿ç”¨æœ€åŸºæœ¬è¯·æ±‚æ–¹æ³•...")
    
    total_ips = 0
    
    for page in range(1, 5):
        print(f"  ğŸ“„ æŠ“å–ç¬¬ {page} é¡µ...")
        
        url = f"https://tonkiang.us/iptvmulticast.php?page={page}&iphone16=&code="
        
        try:
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                ips_found = parse_page(soup, {})
                total_ips += ips_found
                print(f"    âœ… ç¬¬ {page} é¡µæ‰¾åˆ° {ips_found} ä¸ªIP")
            else:
                print(f"    âŒ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥")
                
            time.sleep(1)
            
        except Exception as e:
            print(f"    âŒ ç¬¬ {page} é¡µå¼‚å¸¸: {e}")
            continue
    
    return total_ips

def parse_page(soup, all_ips_by_province):
    """è§£æé¡µé¢å†…å®¹"""
    ips_found = 0
    
    # æ–¹æ³•1: æŸ¥æ‰¾result div
    result_divs = soup.find_all('div', class_='result')
    
    for i, result in enumerate(result_divs):
        ip = extract_ip_from_result(result)
        if ip:
            province = extract_province_from_result(result)
            
            if province not in all_ips_by_province:
                all_ips_by_province[province] = []
            all_ips_by_province[province].append(ip)
            ips_found += 1
            print(f"      âœ… æ‰¾åˆ°IP: {ip} - çœä»½: {province}")
    
    # æ–¹æ³•2: ç›´æ¥åœ¨æ•´ä¸ªé¡µé¢ä¸­æœç´¢IP
    if ips_found == 0:
        page_text = soup.get_text()
        ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
        all_ips = re.findall(ip_pattern, page_text)
        
        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æœåŠ¡å™¨IPçš„åœ°å€
        valid_ips = [ip for ip in all_ips if not ip.startswith(('0.', '127.', '169.', '192.168.', '10.', '172.'))]
        
        if valid_ips:
            print(f"      é€šè¿‡æ­£åˆ™æ‰¾åˆ° {len(valid_ips)} ä¸ªIP: {valid_ips[:3]}...")
            # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å¤„ç†è¿™äº›IP
    
    return ips_found

def extract_ip_from_result(result):
    """ä»result divä¸­æå–IPåœ°å€"""
    ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
    
    # ä»channel divä¸­æå–
    channel_div = result.find('div', class_='channel')
    if channel_div:
        channel_text = channel_div.get_text()
        ip_matches = re.findall(ip_pattern, channel_text)
        if ip_matches:
            return ip_matches[0]
    
    # ä»é“¾æ¥ä¸­æå–
    links = result.find_all('a')
    for link in links:
        href = link.get('href', '')
        link_text = link.get_text()
        
        # ä»hrefä¸­æå–
        ip_matches = re.findall(ip_pattern, href)
        if ip_matches:
            return ip_matches[0]
        
        # ä»é“¾æ¥æ–‡æœ¬ä¸­æå–
        ip_matches = re.findall(ip_pattern, link_text)
        if ip_matches:
            return ip_matches[0]
    
    # ä»æ•´ä¸ªresultä¸­æå–
    result_text = result.get_text()
    ip_matches = re.findall(ip_pattern, result_text)
    if ip_matches:
        return ip_matches[0]
    
    return None

def extract_province_from_result(result):
    """ä»result divä¸­æå–çœä»½ä¿¡æ¯"""
    # æŸ¥æ‰¾åŒ…å«ä½ç½®ä¿¡æ¯çš„div
    location_divs = result.find_all('div', style=re.compile(r'font-size: 11px; color: #aaa;'))
    
    for location_div in location_divs:
        location_text = location_div.get_text()
        
        # æå–çœä»½ä¿¡æ¯
        province_matches = re.findall(r'([\u4e00-\u9fa5]{2,6}çœ|[\u4e00-\u9fa5]{2,4}å¸‚|[\u4e00-\u9fa5]{2,6}è‡ªæ²»åŒº)', location_text)
        if province_matches:
            return province_matches[0]
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çœä»½ï¼Œå°è¯•ä»å…¶ä»–æ–‡æœ¬ä¸­æå–
        if 'ç”µä¿¡' in location_text or 'è”é€š' in location_text or 'ç§»åŠ¨' in location_text:
            # å°è¯•æå–åŸå¸‚
            city_matches = re.findall(r'([\u4e00-\u9fa5]{2,4}å¸‚)', location_text)
            if city_matches:
                return city_matches[0]
    
    return "å…¶ä»–"

def save_results(all_ips_by_province, total_ips):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    output_dir = "tonkiang"
    files_created = 0
    
    print(f"\nğŸ“ˆ æŠ“å–ç»Ÿè®¡:")
    print(f"  æ€»IPæ•°é‡: {total_ips}")
    print(f"  çœä»½æ•°é‡: {len(all_ips_by_province)}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°IPï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ç»“æœæ–‡ä»¶
    if total_ips == 0:
        no_result_file = os.path.join(output_dir, "æ— ç»“æœ.txt")
        with open(no_result_file, 'w', encoding='utf-8') as f:
            f.write("æœ¬æ¬¡æŠ“å–æœªæ‰¾åˆ°ä»»ä½•IPåœ°å€\n")
        files_created = 1
        print(f"  ğŸ’¾ åˆ›å»ºç©ºç»“æœæ–‡ä»¶: {no_result_file}")
    else:
        # ä¿å­˜æ‰¾åˆ°çš„IP
        for province, ips in all_ips_by_province.items():
            safe_filename = re.sub(r'[<>:"/\\|?*]', '', province)
            file_path = os.path.join(output_dir, f"{safe_filename}.txt")
            
            # å»é‡å¹¶å†™å…¥æ–‡ä»¶
            unique_ips = list(set(ips))
            with open(file_path, 'w', encoding='utf-8') as f:
                for ip in unique_ips:
                    f.write(ip + '\n')
            
            files_created += 1
            print(f"  ğŸ’¾ å·²ä¿å­˜ {len(unique_ips)} ä¸ªIPåˆ° {file_path}")
    
    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼")
    print(f"  åˆ›å»ºæ–‡ä»¶æ•°: {files_created}")
    print(f"  æ€»å”¯ä¸€IPæ•°: {total_ips}")
    
    return total_ips, files_created

def alternative_scraping():
    """å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœä¸»ç½‘ç«™æ— æ³•æŠ“å–ï¼Œå°è¯•å…¶ä»–IPTVèµ„æº"""
    print("\nğŸ”„ å°è¯•å¤‡é€‰IPTVèµ„æº...")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–IPTVç½‘ç«™çš„æŠ“å–é€»è¾‘
    # ä¾‹å¦‚ï¼šhttps://iptv-org.github.io/iptv/index.m3u
    # æˆ–è€…ï¼šhttps://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u
    
    print("ğŸ“ å¤‡é€‰èµ„æºåŠŸèƒ½å¾…å®ç°")
    return 0, 0

if __name__ == "__main__":
    try:
        print("=" * 50)
        print("ğŸ¤– IPTV IPåœ°å€æŠ“å–å·¥å…·")
        print("=" * 50)
        
        total_ips, files_created = scrape_tonkiang()
        
        # å¦‚æœä¸»æ–¹æ³•å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ
        if total_ips == 0:
            print("\nğŸ”„ ä¸»ç½‘ç«™æŠ“å–å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ...")
            total_ips, files_created = alternative_scraping()
        
        # æœ€ç»ˆç»“æœ
        if total_ips > 0:
            print(f"\nâœ… æˆåŠŸæŠ“å– {total_ips} ä¸ªIPåœ°å€ï¼Œä¿å­˜åœ¨ {files_created} ä¸ªæ–‡ä»¶ä¸­")
            sys.exit(0)
        else:
            print(f"\nâŒ æœªèƒ½æŠ“å–åˆ°ä»»ä½•IPåœ°å€")
            # åˆ›å»ºæ ‡è®°æ–‡ä»¶
            with open("SCRAPE_FAILED.txt", "w") as f:
                f.write(f"æŠ“å–å¤±è´¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nğŸ’¥ è„šæœ¬æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
