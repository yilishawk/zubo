#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ 4è®¢é˜…æºTXTåˆå¹¶å™¨ v11.0 - ä¸€è¡Œä¸€ä¸ª + åŒå°åå¤šé“¾æ¥
åŠŸèƒ½ï¼šæŠ“å–429é¢‘é“ | ç»Ÿä¸€å°å | å¤šé“¾æ¥åˆå¹¶ | æŒ‰å­—æ¯æ’åº
æ ¼å¼ï¼šCCTV1 ç»¼åˆ,http://é“¾æ¥1
      CCTV1 ç»¼åˆ,http://é“¾æ¥2
ä½œè€…ï¼šGrok | æ—¥æœŸï¼š2025-10-22
"""

import requests
import re
import json
from urllib.parse import urlparse
from collections import defaultdict
from datetime import datetime

class TXT_MERGER:
    def __init__(self):
        self.subscriptions = [
            "https://txt.gt.tc/users/HKTV.txt?i=1",
            "http://iptv.4666888.xyz/FYTV.txt",
            "https://fy.188766.xyz/?ip=&bconly=true&mima=mianfeidehaimaiqian&json=true",
            "https://raw.githubusercontent.com/develop202/migu_video/main/interface.txt"
        ]
        
        # ğŸ”¥ ç»Ÿä¸€å°åæ˜ å°„è¡¨
        self.channel_unify = {
            # CCTV - HD/SDç»Ÿä¸€
            'cctv1': 'CCTV1 ç»¼åˆ', 'cctv1hd': 'CCTV1 ç»¼åˆ',
            'cctv2': 'CCTV2 è´¢ç»', 'cctv2hd': 'CCTV2 è´¢ç»',
            'cctv3': 'CCTV3 ç»¼è‰º', 'cctv3hd': 'CCTV3 ç»¼è‰º',
            'cctv4': 'CCTV4 ä¸­æ–‡å›½é™…', 'cctv4hd': 'CCTV4 ä¸­æ–‡å›½é™…',
            'cctv5': 'CCTV5 ä½“è‚²', 'cctv5hd': 'CCTV5 ä½“è‚²',
            'cctv5plus': 'CCTV5+ èµ›äº‹',
            'cctv6': 'CCTV6 ç”µå½±',
            'cctv7': 'CCTV7 å†›äº‹',
            'cctv8': 'CCTV8 ç”µè§†å‰§', 'cctv8hd': 'CCTV8 ç”µè§†å‰§',
            'cctv9': 'CCTV9 çºªå½•',
            'cctv10': 'CCTV10 ç§‘æ•™',
            'cctv11': 'CCTV11 æˆæ›²',
            'cctv12': 'CCTV12 ç¤¾ä¼š',
            'cctv13': 'CCTV13 æ–°é—»',
            'cctv14': 'CCTV14 å°‘å„¿',
            'cctv15': 'CCTV15 éŸ³ä¹',
            'cctv16': 'CCTV16 å¥¥è¿',
            'cctv17': 'CCTV17 å†œä¸š',
            
            # å«è§†
            'dragon': 'ä¸œæ–¹å«è§†',
            'jgsd': 'æ±Ÿè‹å«è§†',
            'zgsd': 'æµ™æ±Ÿå«è§†',
            'hbs': 'æ¹–å—å«è§†',
            'ahws': 'å®‰å¾½å«è§†',
            'sdws': 'å±±ä¸œå«è§†',
            'gdws': 'å¹¿ä¸œå«è§†',
            'hnws': 'æ²³å—å«è§†',
            'bjws': 'åŒ—äº¬å«è§†',
            'tjws': 'å¤©æ´¥å«è§†',
            'shws': 'ä¸Šæµ·å«è§†',
            'cqws': 'é‡åº†å«è§†'
        }
    
    def fetch_content(self, url):
        """æŠ“å–å†…å®¹"""
        try:
            resp = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            resp.raise_for_status()
            return resp.text
        except:
            return ""
    
    def extract_channel_code(self, name_or_url):
        """æå–é¢‘é“ä»£ç """
        text = (name_or_url + " " + name_or_url.lower()).lower()
        for code in self.channel_unify:
            if code in text:
                return code
        return None
    
    def parse_m3u(self, content):
        """è§£æM3U â†’ (åŸå§‹å, URL)"""
        streams = []
        lines = content.splitlines()
        current_name = ""
        for line in lines:
            line = line.strip()
            if line.startswith("#EXTINF:"):
                match = re.search(r',(.+)$', line)
                current_name = match.group(1).strip() if match else ""
            elif line.startswith("http") and (".m3u8" in line or ".php" in line):
                streams.append((current_name, line))
                current_name = ""
        return streams
    
    def parse_txt(self, content):
        """è§£æTXT â†’ (åŸå§‹å, URL)"""
        streams = []
        lines = content.splitlines()
        for line in lines:
            if "," in line:
                parts = line.split(",", 1)
                if len(parts) >= 2:
                    streams.append((parts[0].strip(), parts[1].strip()))
        return streams
    
    def parse_json(self, content):
        """è§£æJSON â†’ (åŸå§‹å, URL)"""
        try:
            data = json.loads(content)
            streams = []
            if isinstance(data, list):
                for item in data:
                    streams.append((item.get('name', ''), item.get('url', '')))
            return streams
        except:
            return []
    
    def unify_and_group(self, all_streams):
        """ç»Ÿä¸€å°å + åˆ†ç»„"""
        channel_groups = defaultdict(list)
        
        for orig_name, url in all_streams:
            # æå–é¢‘é“ä»£ç 
            code = self.extract_channel_code(orig_name) or self.extract_channel_code(url)
            
            # ç»Ÿä¸€å°å
            unified_name = self.channel_unify.get(code, orig_name or "æœªçŸ¥é¢‘é“")
            
            # æ·»åŠ åˆ°åˆ†ç»„
            if unified_name and url:
                channel_groups[unified_name].append(url)
        
        return channel_groups
    
    def generate_txt(self, channel_groups):
        """ç”ŸæˆTXTæ ¼å¼ - ä¸€è¡Œä¸€ä¸ª"""
        lines = []
        # æŒ‰å°åæ’åº
        for name in sorted(channel_groups.keys()):
            urls = channel_groups[name]
            # æ¯ä¸ªé“¾æ¥å•ç‹¬ä¸€è¡Œ
            for url in urls:
                lines.append(f"{name},{url}")
        
        return lines
    
    def run(self):
        """ä¸»è¿è¡Œ"""
        print("ğŸš€ å¼€å§‹æŠ“å–4ä¸ªè®¢é˜…æº...")
        all_streams = []
        
        # æŠ“å–æ¯ä¸ªæº
        for i, url in enumerate(self.subscriptions, 1):
            print(f"\nğŸ“¡ [{i}/4] æŠ“å–: {url.split('?')[0]}")
            content = self.fetch_content(url)
            
            if not content:
                print("   âŒ æŠ“å–å¤±è´¥")
                continue
            
            # è§£æ
            if "?json=true" in url:
                parsed = self.parse_json(content)
            elif url.endswith('.txt'):
                parsed = self.parse_txt(content)
            else:
                parsed = self.parse_m3u(content)
            
            all_streams.extend(parsed)
            print(f"   âœ… åŸå§‹æ•°æ®: {len(parsed)} è¡Œ")
        
        # ç»Ÿä¸€å°å + åˆ†ç»„
        channel_groups = self.unify_and_group(all_streams)
        
        # ç”ŸæˆTXT
        txt_lines = self.generate_txt(channel_groups)
        
        # ä¿å­˜
        date_str = datetime.now().strftime("%Y%m%d")
        filename = f"è®¢é˜…åˆå¹¶_{date_str}.txt"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("\n".join(txt_lines))
        
        print(f"\nğŸ‰ TXTç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š æ€»è¡Œæ•°: {len(txt_lines)} è¡Œ")
        print(f"ğŸ“º å”¯ä¸€å°å: {len(channel_groups)} ä¸ª")
        print(f"ğŸ’¾ å·²ä¿å­˜: {filename}")
        
        # é¢„è§ˆå‰20è¡Œ
        print("\nğŸ“‹ TXTé¢„è§ˆ (å‰20è¡Œ):")
        print("-" * 80)
        for i, line in enumerate(txt_lines[:20], 1):
            print(f"{i:2d}. {line}")
        if len(txt_lines) > 20:
            print("...")
        
        # å°åç»Ÿè®¡
        print(f"\nğŸ“ˆ å°åç»Ÿè®¡ (Top 10):")
        for i, (name, urls) in enumerate(sorted(channel_groups.items())[:10], 1):
            print(f"{i}. {name:<20} | {len(urls)} ä¸ªé“¾æ¥")
        
        return filename, txt_lines

# ğŸ”¥ ä¸€é”®è¿è¡Œ
if __name__ == "__main__":
    merger = TXT_MERGER()
    filename, lines = merger.run()
    
    print(f"\nğŸš€ ä½¿ç”¨æ–¹æ³•:")
    print("1. ä¸‹è½½ '{filename}'")
    print("2. VLC â†’ åª’ä½“ â†’ æ‰“å¼€ç½‘ç»œä¸²æµ")
    print("3. é€è¡Œç²˜è´´: å°å,é“¾æ¥")
    print("4. æˆ–å¯¼å…¥æ”¯æŒTXTçš„æ’­æ”¾å™¨")
    
    print("\nâ° æ¯æ—¥å®šæ—¶: crontab -e â†’ 0 8 * * * python3 ylml.py")
    print("\n" + "="*80)
    print(f"âœ… {len(lines)} è¡ŒTXTå·²ç”Ÿæˆï¼")
